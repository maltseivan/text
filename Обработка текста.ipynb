{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfeff88e",
   "metadata": {},
   "source": [
    "### Обработка текста по модулю spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12eef0a3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\IVANMA~1\\AppData\\Local\\Temp/ipykernel_12152/1205027368.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mspacy\u001b[0m \u001b[1;31m# подключим библиотеку\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Загрузим NLP-модель для английского языка\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mnlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'en_core_web_lg'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# en_core_web_lg это название модели, которая была скачена и установлена\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "import spacy # подключим библиотеку\n",
    "\n",
    "# Загрузим NLP-модель для английского языка\n",
    "nlp = spacy.load('en_core_web_lg') # en_core_web_lg это название модели, которая была скачена и установлена\n",
    "\n",
    "# Текст для анализа. Можете написать свой текст (на английском)\n",
    "text = \"\"\"London is the capital and most populous city of England and \n",
    "the United Kingdom.  Standing on the River Thames in the south east \n",
    "of the island of Great Britain, London has been a major settlement \n",
    "for two millennia. It was founded by the Romans, who named it Londinium.\n",
    "\"\"\"\n",
    "\n",
    "# Парсинг текста с помощью spaCy. Эта команда запускает целый конвейер по обработке текста\n",
    "doc = nlp(text)\n",
    "\n",
    "# в переменной 'doc' теперь содержится обработанная версия текста\n",
    "# мы можем делать с ней все что угодно!\n",
    "# например, распечатать все обнаруженные именованные сущности (в .ents)\n",
    "for entity in doc.ents:\n",
    "    print(f\"{entity.text} ({entity.label_})\")# печатаем слово .text (словосочетание) и его тип .label_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad27861",
   "metadata": {},
   "source": [
    "### Скачиваем библиотеку NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ccc9ae8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Ivan\n",
      "[nltk_data]     Maltsev\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "#import nltk.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d327378d",
   "metadata": {},
   "source": [
    "### Токенизация по предложения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ace44066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We try to implement NLTK.Sent_tokenize.\n",
      "\n",
      "It is very hard to produce good tokens.\n",
      "\n",
      "Our approach is model-based one!\n",
      "\n",
      "And they has already train a good model for tokenizing.\n",
      "\n",
      "Really?\n",
      "\n",
      "Yes ... try.\n",
      "\n",
      "Hard words: vice president, half sister\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"We try to implement NLTK.Sent_tokenize. It is very hard to produce good tokens. Our approach is model-based one! And they has already train a good model for tokenizing. Really? Yes ... try. Hard words: vice president, half sister\"\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "for sentence in sentences:\n",
    "    print(sentence)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb1f73c",
   "metadata": {},
   "source": [
    "### Токенизация по словам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38b2fd66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['We', 'try', 'to', 'implement', 'NLTK.Sent_tokenize', '.']\n",
      "\n",
      "['It', 'is', 'very', 'hard', 'to', 'produce', 'good', 'tokens', '.']\n",
      "\n",
      "['Our', 'approach', 'is', 'model-based', 'one', '!']\n",
      "\n",
      "['And', 'they', 'has', 'already', 'train', 'a', 'good', 'model', 'for', 'tokenizing', '.']\n",
      "\n",
      "['Really', '?']\n",
      "\n",
      "['Yes', '...', 'try', '.']\n",
      "\n",
      "['Hard', 'words', ':', 'vice', 'president', ',', 'half', 'sister']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sentence in sentences:\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    print(words)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae62832",
   "metadata": {},
   "source": [
    "### Лематизация и стеминг текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75c737a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Ivan\n",
      "[nltk_data]     Maltsev\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmer: seek\n",
      "Lemmatizer: seek\n",
      "\n",
      "Stemmer: drove\n",
      "Lemmatizer: drive\n",
      "\n",
      "Stemmer: meet\n",
      "Lemmatizer: meeting\n",
      "\n",
      "Stemmer: meet\n",
      "Lemmatizer: meet\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def compare_stemmer_and_lemmatizer(stemmer, lemmatizer, word, pos):\n",
    "    \"\"\"\n",
    "    Print the results of stemming and lemmitization using the passed stemmer, lemmatizer, word and pos (part of speech)\n",
    "    \"\"\"\n",
    "    print(\"Stemmer:\", stemmer.stem(word))\n",
    "    print(\"Lemmatizer:\", lemmatizer.lemmatize(word, pos))\n",
    "    print()\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "compare_stemmer_and_lemmatizer(stemmer, lemmatizer, word = \"seeking\", pos = wordnet.VERB)\n",
    "compare_stemmer_and_lemmatizer(stemmer, lemmatizer, word = \"drove\", pos = wordnet.VERB)\n",
    "compare_stemmer_and_lemmatizer(stemmer, lemmatizer, word = \"meeting\", pos = wordnet.NOUN)\n",
    "compare_stemmer_and_lemmatizer(stemmer, lemmatizer, word = \"meeting\", pos = wordnet.VERB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ab9855",
   "metadata": {},
   "source": [
    "### Стоп-слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b290f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Ivan\n",
      "[nltk_data]     Maltsev\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdda4bc",
   "metadata": {},
   "source": [
    "Убираем стоп-слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fcf8ac3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Backgammon', 'one', 'oldest', 'known', 'board', 'games', '.']\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words(\"english\")) # получим стоп-слова, превратим их в множество с помощью set()\n",
    "sentence = \"Backgammon is one of the oldest known board games.\" # зададим строку\n",
    "\n",
    "words = nltk.word_tokenize(sentence) # токенизируем ее по словам\n",
    "\n",
    "# и будем в цикле перебирать все слова из words, проверять входит ли оно в множество стоп-слов stop_words,\n",
    "# и если нет, то вернем слово word, иначе ничего не вернем. \n",
    "without_stop_words = [word for word in words if not word in stop_words] \n",
    "print(without_stop_words) #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a672bbac",
   "metadata": {},
   "source": [
    "### Чистка текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed48e230",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We try to implement NLTK.Sent_tokenize. It is very hard to produce good tokens. Our approach is model-based one! And they has already train a good model for tokenizing. Really? Yes ... try. Hard words: vice president, half sister'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac63298",
   "metadata": {},
   "outputs": [],
   "source": [
    "text.replace('это', '') # заменяем все подстроки 'это'  на пробел.\n",
    "text = text.replace('это', '') # если хотим поменять - надо переприсвоить ей значение."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
